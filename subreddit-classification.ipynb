{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Downloading and loading Data\n\nThis code loads the prepared split of the Reddit data into training, validation and testing set.","metadata":{"id":"DyFYH3v7-ix0"}},{"cell_type":"code","source":"!wget -O reddit_data_split.zip https://gla-my.sharepoint.com/:u:/g/personal/jake_lever_glasgow_ac_uk/EapVNOIV84tPnQuuFBNgG9UBYIWipQ9JL4QTfSgRtIacBw?download=1\n!unzip -o reddit_data_split.zip","metadata":{"id":"tG_zJqYH9sTd","outputId":"cba6b5e6-ab81-479c-b1e5-a3a1c9d17018","scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Downloaded Data is divided into 3 sets of data\n1. Train Dataset\n2. Test Dataset\n3. Validation Dataset","metadata":{"id":"Zp_SJ-2iz9vl"}},{"cell_type":"code","source":"import json\n\nwith open('reddit_train.json') as f:\n    train_data = json.load(f)\nwith open('reddit_val.json') as f:\n    validation_data = json.load(f)\nwith open('reddit_test.json') as f:\n    test_data = json.load(f)\n\nprint(\"Number of posts in training data:\", len(train_data))\nprint(\"Number of posts in validation data:\", len(validation_data))\nprint(\"Number of posts in test data:\", len(test_data))","metadata":{"id":"xCezGTgdRYkj","outputId":"dee3fed1-656b-407c-d7e4-ff100d00b68b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Importing Required Packages\n","metadata":{"id":"QH3Ibv5O6jKp"}},{"cell_type":"code","source":"#!pip install -U spacy\nfrom sklearn import metrics\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.naive_bayes import MultinomialNB\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport spacy\nimport requests\nimport time\nimport pandas as pd\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus.reader.knbc import test\nimport pandas as pd\n\n# Load the medium english model. \n# We will use this model to get embedding features for tokens later.\n#!python -m spacy download en_core_web_md\n\nnlp = spacy.load('en_core_web_sm', disable=['ner'])\nnlp.remove_pipe('tagger')\nnlp.remove_pipe('parser')\n\nnltk.download('stopwords')\nstop_words = nltk.corpus.stopwords.words('english')\n","metadata":{"id":"Q-677WyMjcU-","outputId":"9dd6038b-8316-4e7d-ae73-7c3f44ee3e09"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{"id":"3jlQ6J6h6jKt"}},{"cell_type":"markdown","source":"### Cleaning/Pre-processing Data","metadata":{"id":"YI5qU08h6jKu"}},{"cell_type":"code","source":"\n#Respective dataframes are created for each dataset\ntrain_data = pd.DataFrame(train_data)\nvalidation_data = pd.DataFrame(validation_data)\ntest_data = pd.DataFrame(test_data)\n\n#removed duplicate titles\ntrain_data.drop_duplicates(subset='title', inplace = True)\nvalidation_data.drop_duplicates(subset='title', inplace = True)\ntest_data.drop_duplicates(subset='title', inplace = True)\n\n# Check for the Null Entries\nprint(\"Train - Null Entries:\")\nprint(train_data.isnull().sum())\nprint(\"Validation - Null Entries:\")\nprint(validation_data.isnull().sum())\nprint(\"Test - Null Entries:\")\nprint(test_data.isnull().sum())","metadata":{"id":"VSUmM7djTEIu","outputId":"e611612e-4a62-4b46-bda9-bd44460888fa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Printing first few rows of the train dataset","metadata":{"id":"KwU2nAxL62wY"}},{"cell_type":"code","source":"train_data.head()","metadata":{"id":"oFZ61GiG98Nm","outputId":"5af74846-4e9f-4a73-e3b5-f7b065e2f790"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Histogram of train Dataset","metadata":{"id":"gf7z2dhG7AOW"}},{"cell_type":"code","source":"train_data['subreddit'].value_counts().plot.bar()","metadata":{"id":"FPDY7bHICMAh","outputId":"914c2395-ee11-44e3-8cd8-1f386cc09775"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Histogram of Test Dataset","metadata":{"id":"U-ILPX_D6Hgj"}},{"cell_type":"code","source":"test_data['subreddit'].value_counts().plot.bar()","metadata":{"id":"QP7AQu5g6Aos","outputId":"05886027-36b3-414e-f35f-7bb785042f8a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Histogram of Validation Dataset","metadata":{"id":"RNYkoffP6RhB"}},{"cell_type":"code","source":"validation_data['subreddit'].value_counts().plot.bar()","metadata":{"id":"xgixpabm6Vs9","outputId":"e28249c2-1c83-4f96-8586-12bfd7e57bed"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Mapping unique numbers to each subreddit label","metadata":{"id":"bB0Ucg7z7YZA"}},{"cell_type":"code","source":"unique_labels = train_data.subreddit.unique()\nprint(unique_labels)\nunique_labels_dict = {}\nunique_labels_rev_dict = {}\n\nfor i,l in enumerate(unique_labels):\n  unique_labels_dict[l] = i\n  unique_labels_rev_dict[i] = l\nprint(unique_labels_dict)\nprint(unique_labels_rev_dict)","metadata":{"id":"CYysqqq2f0ku","outputId":"17d7b836-6954-4654-dfef-8842474aa116"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['labels'] = train_data['subreddit'].map(unique_labels_dict)\nvalidation_data['labels'] = validation_data['subreddit'].map(unique_labels_dict)\ntest_data['labels'] = test_data['subreddit'].map(unique_labels_dict)\n\n#train_data","metadata":{"id":"CndqUZFyFEfk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"id":"2cqGIlaTvp4G","outputId":"5f590383-0653-4105-b103-fded62b3d6c3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting Train, Test and Validation datasets to X and Y\n\nHere X would be body, and Y would be the subreddit we are trying to predict/classify","metadata":{"id":"U4Cg8uLR7sOr"}},{"cell_type":"code","source":"X_train = train_data['body']\nY_train = train_data['labels']\n\nX_val = validation_data['body']\nY_val = validation_data['labels']\n\nX_test = test_data['body']\nY_test = test_data['labels']","metadata":{"id":"CHCv0ZUD8ab0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A function to Tokenize text","metadata":{"id":"JPwChU-s8Fwi"}},{"cell_type":"code","source":"def text_pipeline_spacy(text):\n    tokens = []\n    doc = nlp(text)\n    for t in doc:\n        if not t.is_stop and not t.is_punct and not t.is_space and t.lemma_.isalnum():\n            tokens.append(t.lemma_.lower())\n    tokens = ' '.join([char for char in tokens])\n    return tokens\n","metadata":{"id":"UKZoI5ZChFx5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The posts in each subreddit is tokenized","metadata":{"id":"cOP0_sMF8PtQ"}},{"cell_type":"code","source":"X_train_tokenized = [ text_pipeline_spacy(x) for x in X_train ]\nX_train_tokenized[0:5]","metadata":{"id":"JDUd2B3coyu7","outputId":"6e12fac5-4bb9-4334-a753-19a58b97e200"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Q1:\n\nUse the text from the reddit posts (known as “body”) to train classification models using the Scikit Learn package. The labels to predict are the subreddit for each post. Conduct experiments using the following combinations of classifier models and feature representations:\n1. Dummy Classifier with strategy=\"most_frequent\"\n2. Dummy Classifier with strategy=\"stratified\"\n3. LogisticRegression with One-hot vectorization \n4. LogisticRegression with TF-IDF vectorization (default settings)\n5. SVC Classifier with  One-hot vectorization (SVM with RBF kernel, default settings))\n","metadata":{"id":"7txBmrCG_tKr"}},{"cell_type":"markdown","source":"### Importing Required Packages","metadata":{"id":"z-mwlU6j6jK7"}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score","metadata":{"id":"iuF14A_SNDmX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transforming given text to Vectors based on the count of each word that occurs in the entire text","metadata":{"id":"bfcEpK0s-hCX"}},{"cell_type":"code","source":"OH_transformer = CountVectorizer() #creates vectors on the basis of frequency of the occurence term\nOH_transformer.fit(X_train_tokenized)\n\nOHV_train = OH_transformer.transform(X_train)\nOHV_test = OH_transformer.transform(X_test)\nOHV_valid = OH_transformer.transform(X_val)","metadata":{"id":"TDIkMA8k7ySn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transforming given text to Vectors based on the TF-IDF weights","metadata":{"id":"pWj3L4Ex_X7N"}},{"cell_type":"code","source":"tfidf_transformer = TfidfVectorizer() #creates vectors on the basis of TF-IDF\ntfidf_transformer.fit(X_train_tokenized)\n\ntfidf_vec_train = tfidf_transformer.transform(X_train)\ntfidf_vec_test = tfidf_transformer.transform(X_test)\ntfidf_vec_valid = tfidf_transformer.transform(X_val)","metadata":{"id":"8c2uIUhD_r_t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Q1a:\nAn important first step for any machine learning project is to explore the dataset. Calculate counts for the various\nlabels and comment on the distribution of labels in the training/validation/test sets","metadata":{"id":"iK9u5izk_wSh"}},{"cell_type":"markdown","source":"### Count of posts per subreddit for different datasets","metadata":{"id":"3sKAtszC_iFT"}},{"cell_type":"code","source":"train_data['subreddit'].value_counts().plot.bar()","metadata":{"id":"_J90sKG2HDoD","outputId":"25e1c153-baa0-4ca0-ff6c-9588b636b0b8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data['subreddit'].value_counts().plot.bar()","metadata":{"id":"22OryawKq0nl","outputId":"d433e1d8-4820-4dba-ccb9-cb0d94b53fb6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_data['subreddit'].value_counts().plot.bar()","metadata":{"id":"KVLnOvzJq-kM","outputId":"607ff17b-6125-4097-b976-45e2b50c9926"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data\nsubreddit = train_data.subreddit.unique().tolist()\ndf_subr = train_data['subreddit'].value_counts()\ndf_subr.to_frame()\ndf_subr","metadata":{"id":"oEcG4KkHDIL7","outputId":"8420117f-a829-40f7-b1ed-704f87aad003"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data\nsubreddit = test_data.subreddit.unique().tolist()\ndf_subr_t = test_data['subreddit'].value_counts()\ndf_subr_t.to_frame()\ndf_subr_t","metadata":{"id":"MJYEldkTEGTz","outputId":"83b7b393-b82d-4878-9424-2516a5bc881a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subreddit = validation_data.subreddit.unique().tolist()\ndf_subr_v = validation_data['subreddit'].value_counts()\ndf_subr_v.to_frame()\ndf_subr_v","metadata":{"id":"F4yNasVQEhIw","outputId":"0cd98bc0-9020-47f1-9321-91bde869fd0b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Q1b:\nImplement the five classifiers above, train them on the training set and evaluate on the test set. Discuss the classifier performance in comparison to the others and preprocessing techniques.\n\nFor the above classifiers report the classifier accuracy as well as macro/weighted-averaged precision, recall, and F1 (to three decimal places). Show the overall results  obtained by the classifiers on the training and test sets in one table, and highlight the best performance. For the best performing classifier (by weighted F1 in test set) Include a bar chart graph with the F1 score for each class - (subreddits on x-axis, F1 score on Y axis).\nAnalyse and discuss the effectiveness of the classifiers. Your discussion should include how the models perform relative to the baselines and each other. It should discuss the classifiers’ behaviours with respect to: \n1. Appropriate model “fit” (how well is the model fit to the training/test dataset),\n2. Dataset considerations (e.g. how are labels distributed, any other dataset issues?)\n3. Classifier models (and their key parameters).\n","metadata":{"id":"TQaSIxai_8dQ"}},{"cell_type":"code","source":"classifier_scores = pd.DataFrame(columns=['classifier', 'accuracy', 'precision', 'recall', 'f1_score'])","metadata":{"id":"sFXIba2T2Zpn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dummy Classifier \n\nwith  strategy most_frequent","metadata":{"id":"Fv5HlH2Rs1SH"}},{"cell_type":"code","source":"dummy_mf_model = DummyClassifier(strategy='most_frequent')\ndummy_mf_model.fit(OHV_train, Y_train)\npreds = dummy_mf_model.predict(OHV_test)\nprint(classification_report(Y_test, preds))\n\n\nclassifier_scores = classifier_scores.append({'classifier' : 'Dummy_classifier_Most_frequent',\n                          'accuracy' : accuracy_score(Y_test, preds),\n                          'precision' : precision_score(Y_test, preds, average='weighted'),\n                          'recall' : recall_score(Y_test, preds, average='weighted'),\n                          'f1_score' : f1_score(Y_test, preds, average='weighted')}, ignore_index=True)","metadata":{"id":"OCoUgSHOxAF6","outputId":"0590a9ef-b719-4dde-e6f3-a3d5e44eb651"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dummy Classifier \n\nwith  strategy stratified","metadata":{"id":"vbsLeueOvIRj"}},{"cell_type":"code","source":"dummy_mf_model = DummyClassifier(strategy='stratified')\ndummy_mf_model.fit(OHV_train, Y_train)\npreds = dummy_mf_model.predict(OHV_test)\nprint(classification_report(Y_test, preds))\n\nclassifier_scores = classifier_scores.append({'classifier' : 'Dummy_classifier_stratified',\n                          'accuracy' : accuracy_score(Y_test, preds),\n                          'precision' : precision_score(Y_test, preds, average='weighted'),\n                          'recall' : recall_score(Y_test, preds, average='weighted'),\n                          'f1_score' : f1_score(Y_test, preds, average='weighted')}, ignore_index=True)","metadata":{"id":"xDNZorRzxubT","outputId":"d534e02c-b314-45e0-c1d3-4fa49054f24c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression\n\nwith one-hot Vectorization","metadata":{"id":"wbose6hYvMuj"}},{"cell_type":"code","source":"lr_model_OHV = LogisticRegression(max_iter = 1000)\nlr_model_OHV.fit(OHV_train, Y_train)\npreds = lr_model_OHV.predict(OHV_test)\nprint(classification_report(Y_test, preds))\n\nclassifier_scores = classifier_scores.append({'classifier' : 'LogisticRegression - One Hot Vector',\n                          'accuracy' : accuracy_score(Y_test, preds),\n                          'precision' : precision_score(Y_test, preds, average='weighted'),\n                          'recall' : recall_score(Y_test, preds, average='weighted'),\n                          'f1_score' : f1_score(Y_test, preds, average='weighted')}, ignore_index=True)","metadata":{"id":"sPpzVWdHz0C2","outputId":"950f5b85-6cbd-4b28-957e-a8b03a966b73"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression\n\nwith TF-IDF Vectorization","metadata":{"id":"AdpC7dJuvkYw"}},{"cell_type":"code","source":"lr_model_tfidf = LogisticRegression(max_iter = 1000)\nlr_model_tfidf.fit(tfidf_vec_train, Y_train)\npreds = lr_model_tfidf.predict(tfidf_vec_test)\nreport_lr = classification_report(Y_test, preds, output_dict = True)\nprint(classification_report(Y_test, preds))\n\nclassifier_scores = classifier_scores.append({'classifier' : 'LogisticRegression - TFIDF',\n                          'accuracy' : accuracy_score(Y_test, preds),\n                          'precision' : precision_score(Y_test, preds, average='weighted'),\n                          'recall' : recall_score(Y_test, preds, average='weighted'),\n                          'f1_score' : f1_score(Y_test, preds, average='weighted')}, ignore_index=True)","metadata":{"id":"lamhKHCC0qHe","outputId":"59c2e39b-a8b8-4c54-a62f-fd203c2633bd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SVC Classifier \n\nwith  One-hot vectorization ","metadata":{"id":"2FunIc26vwxk"}},{"cell_type":"code","source":"svc_tfidf_model = SVC(kernel='rbf')\nsvc_tfidf_model.fit(OHV_train, Y_train)\npreds = svc_tfidf_model.predict(OHV_test)\nprint(classification_report(Y_test, preds))\n\nclassifier_scores = classifier_scores.append({'classifier' : 'SVC(rbf)',\n                          'accuracy' : accuracy_score(Y_test, preds),\n                          'precision' : precision_score(Y_test, preds, average='weighted'),\n                          'recall' : recall_score(Y_test, preds, average='weighted'),\n                          'f1_score' : f1_score(Y_test, preds, average='weighted')}, ignore_index=True)","metadata":{"id":"IosfJAUZ0p7I","outputId":"470a44fb-35be-400a-f248-f9911334c1fb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Comparison of performance of all models ","metadata":{"id":"r8jHwwDWFuRV"}},{"cell_type":"code","source":"def highlight_max(s, threshold, column):\n    is_max = pd.Series(data=False, index=s.index)\n    is_max[column] = s.loc[column] >= threshold\n    return ['background-color: yellow'if is_max.any() else''for v in is_max]\n\nclassifier_scores.style.apply(highlight_max, threshold=classifier_scores['f1_score'].max(), column=['f1_score'], axis=1)","metadata":{"id":"CNFiQ4ZSFomN","outputId":"cb5f3b0b-6a45-44bb-a7a0-a4639af3a6fe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### F1 Score for DIfferent labels","metadata":{"id":"b4jnC2QxwTfz"}},{"cell_type":"code","source":"df_lr = pd.DataFrame(report_lr).transpose()\ndf_lr[:-3][\"f1-score\"].plot.bar()","metadata":{"id":"V7RTrs0Qhixm","outputId":"4ab1f999-85ef-4a6c-cb0c-5a304dde1793"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Q1c:\nChoose your own classifier/tokenization/normalisations approach, and report on its performance with respect to the five previous ones on the test set.\nYou should describe your selected classifier and vectorization approach including a justification for its appropriateness.","metadata":{"id":"YBMJdw_j_8fx"}},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(tfidf_vec_train.toarray(), Y_train)\npreds = sgd.predict(tfidf_vec_test.toarray())\nprint(classification_report(Y_test, preds))\n\nclassifier_scores = classifier_scores.append({'classifier' : 'SGDClassifier',\n                          'accuracy' : accuracy_score(Y_test, preds),\n                          'precision' : precision_score(Y_test, preds, average='weighted'),\n                          'recall' : recall_score(Y_test, preds, average='weighted'),\n                          'f1_score' : f1_score(Y_test, preds, average='weighted')}, ignore_index=True)","metadata":{"id":"Oq-OF2MfXMw1","outputId":"1c2d0399-1af1-42eb-cacd-9227344514df"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function that returns the best performing model","metadata":{"id":"4PiMM7r2xOUp"}},{"cell_type":"code","source":"classifier_scores.style.apply(highlight_max, threshold=classifier_scores['f1_score'].max(), column=['f1_score'], axis=1)","metadata":{"id":"1kZRIX-jOEka","outputId":"339651fc-5f9f-4cc8-c1e8-c044edf032a4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Q2: Tuning and Error Analysis\n\nIn this task you will improve the effectiveness of the LogisticRegression with TF-IDF vectorization from Q1. \n\n### Q2a:\n\n**Parameter** tuning - Tune the parameters for both the vectorizer and classifier on the validation set (or using CV-fold validation on the train).\n\n* Classifier - Regularisation C value (typical values might be powers of 10 (from 10^-3 to 10^5)\n* Vectorizer - Parameters: sublinear_tf and max_features (vocabulary size) (in a range None to 50k)\n*  Select another parameter of your choice from the classifier or vectorizer","metadata":{"id":"zZcRes8WAOo6"}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\n\npipe = Pipeline(steps=[('tfidf', TfidfVectorizer()), ('lr', LogisticRegression(max_iter = 1000))])\n\n \n\nparam_grid = {\n    'tfidf__sublinear_tf': [True, False],\n    'tfidf__max_features': [None, 500, 5000, 50000],\n    'lr__C': [0.01, 0.1, 100, 1000, 10000],\n    'lr__class_weight': [None, 'balanced'],\n    'tfidf__ngram_range': [(1,1), (1,2)]\n}\n\nsearch = GridSearchCV(pipe, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=3)\nsearch.fit(X_train_tokenized, Y_train)\nprint(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\nprint(search.best_params_)","metadata":{"id":"C62EVZDapjVe","outputId":"caa953e2-b70a-43c9-f568-c525f63e2552"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_transformer = TfidfVectorizer(max_features=None, ngram_range=(1,2), sublinear_tf=True)\ntfidf_transformer.fit(X_train_tokenized)\n\nX_train_tfidf = tfidf_transformer.transform(X_train_tokenized)\n\nlr_model = LogisticRegression(C=100, class_weight='balanced', max_iter=1000)\nlr_model.fit(X_train_tfidf, Y_train)","metadata":{"id":"rW7_eOJjijsQ","outputId":"9440cac5-953b-4a38-d058-c0341fb09d2e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Q2b:\n\nError analysis - Manually examine the predictions of your optimised classifier on the test set.  Analyse the results for patterns and trends.  Hypothesise why common classification errors are made.  Report on your error analysis process and summarise your findings. ","metadata":{"id":"pIsRTiW-ABs8"}},{"cell_type":"code","source":"X_test_transformed = tfidf_transformer.transform(X_test)\n\npreds = lr_model.predict(X_test_transformed)\nprint(classification_report(Y_test, preds))\n","metadata":{"id":"L1y-s_V9GEzI","outputId":"6c292e27-868e-421e-beaa-7f580bfad86d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Q3: Feature Engineering \n\nIn this task your goal is to add two features to (try to) improve subreddit classification performance obtained in Q2.\nYou must implement and describe two new classifier features and add them to the tuned model from Q2. Examples include adding other properties of the posts, leveraging embedding-based features, different vectorization approaches, etc, (This is your chance to be creative!). As before, report the results in terms of evaluation metrics on the test data. Additionally, include a well-labelled confusion matrix and discuss the result in reference to Q2 and what helped (or didn’t) and why you think so. In summary: \n","metadata":{"id":"BWs7YqOOAbLp"}},{"cell_type":"markdown","source":"### Q3a:\nPropose two features of your own, along with your rationale behind your choice. ","metadata":{"id":"lOpWB9pMAcri"}},{"cell_type":"code","source":"train_data['post_length'] = [len(x) for x in train_data['body']]\ntest_data['post_length'] = [len(x) for x in test_data['body']]\nvalidation_data['post_length'] = [len(x) for x in validation_data['body']]","metadata":{"id":"GIgQBrDMiFiu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"id":"O7q2E2WbwTjP","outputId":"ebbddf62-c0c7-4504-c38a-63184a34545f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_combined = [ text_pipeline_spacy(x) for x in train_data['body'] + ' ' +train_data['title'] ]\nX_test_combined = [ text_pipeline_spacy(x) for x in test_data['body'] + ' ' +test_data['title'] ]\nX_validation_combined = [ text_pipeline_spacy(x) for x in validation_data['body'] + ' ' +validation_data['title'] ]","metadata":{"id":"ob4Bie13b8bN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Q3b:\nTrain, validate and test models that incorporate combinations of your features, and briefly report on the evaluation metrics ","metadata":{"id":"ymz5BE7NAgZM"}},{"cell_type":"code","source":"tfidf_transformer_final = TfidfVectorizer(max_features=None, ngram_range=(1,2), sublinear_tf=True)\ntfidf_transformer_final.fit(X_train_combined)\n\nX_train_combined = tfidf_transformer_final.transform(X_train_combined)\nX_test_combined = tfidf_transformer_final.transform(X_test_combined)\nX_validation_combined = tfidf_transformer_final.transform(X_validation_combined)\n\nX_train_combined.toarray()","metadata":{"id":"xfTGUtU9ic8o","outputId":"8b41b648-3b8c-4c81-8188-6ce4d1c924d0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom scipy import sparse\nfrom sklearn.preprocessing import normalize\n\nXTRAIN = np.append( X_train_combined.toarray(), np.array(train_data['post_length']).reshape(-1,1),axis=1)\nXTEST = np.append( X_test_combined.toarray(), np.array(test_data['post_length']).reshape(-1,1),axis=1)\nXVALIDATION = np.append( X_validation_combined.toarray(), np.array(validation_data['post_length']).reshape(-1,1),axis=1)\n\n\n\nXTRAIN = sparse.csr_matrix(XTRAIN)\n\n\nXTRAIN.shape\n","metadata":{"id":"EAREo2L-nknq","outputId":"16b1354d-e932-4943-ed25-8ef2c7eafdca"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_model = LogisticRegression(C=100, class_weight='balanced', max_iter=1000)\nlr_model.fit(XTRAIN, Y_train)","metadata":{"id":"BHZO28N-1SpG","outputId":"d23693a1-245e-4519-ab24-3a0220900ce2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = lr_model.predict(XTEST)","metadata":{"id":"Kw62J1tt21Lx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(Y_test, preds))","metadata":{"id":"7w3Wf4mb4Cke","outputId":"a39db1d8-0dda-41d6-dd59-aee958341fea"},"execution_count":null,"outputs":[]}]}